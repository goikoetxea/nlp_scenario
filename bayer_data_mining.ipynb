{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdfminer\n",
    "from PyPDF2 import PdfFileReader\n",
    "import nltk\n",
    "import numpy as np\n",
    "import re\n",
    "import sys\n",
    "reload(sys)\n",
    "sys.setdefaultencoding('utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter#process_pdf\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "from pdfminer.converter import TextConverter\n",
    "from pdfminer.layout import LAParams\n",
    "\n",
    "from cStringIO import StringIO\n",
    "\n",
    "def pdf_to_text(pdfname):\n",
    "\n",
    "    # PDFMiner boilerplate\n",
    "    rsrcmgr = PDFResourceManager()\n",
    "    sio = StringIO()\n",
    "    codec = 'utf-8'\n",
    "    laparams = LAParams()\n",
    "    device = TextConverter(rsrcmgr, sio, codec=codec, laparams=laparams)\n",
    "    interpreter = PDFPageInterpreter(rsrcmgr, device)\n",
    "\n",
    "    # Extract text\n",
    "    fp = file(pdfname, 'rb')\n",
    "    for page in PDFPage.get_pages(fp):\n",
    "        interpreter.process_page(page)\n",
    "    fp.close()\n",
    "\n",
    "    # Get text from StringIO\n",
    "    text = sio.getvalue()\n",
    "\n",
    "    # Cleanup\n",
    "    device.close()\n",
    "    sio.close()\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_specific_section(text, section_name):\n",
    "    \n",
    "    end_section = ''\n",
    "    lines = list(filter(bool,text.split('\\n')))\n",
    "    for i in range(len(lines)):\n",
    "        start_section = '[0-9]+.[0-9]+' + '(.*)  '+  'Quality aspects'\n",
    "        if re.match(start_section, lines[i]):\n",
    "            start_section_number = lines[i].split()[0]\n",
    "            start_section_head = re.findall('[0-9].', start_section_number)[0]\n",
    "            start_section_name = lines[i]\n",
    "            end_section_number = start_section_head + str(int(start_section_number.strip('.')[-1]) + 1)\n",
    "            end = end_section_number + '(.*)'+'  [A-Za-z]*'\n",
    "    end_section += end\n",
    "    end_section_name = []\n",
    "    for i in range(len(lines)):\n",
    "        if re.match(end_section, lines[i]):\n",
    "            end_section_name.append(lines[i])\n",
    "\n",
    "    str1 = text[text.find(start_section_name)+len(start_section_name):text.rfind(end_section_name[0])]\n",
    "    return str1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLUSTER_THRESHOLD = 5\n",
    "TOP_SENTENCES = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _score_sentences(sentences, important_words):\n",
    "    scores = []\n",
    "    sentence_idx = -1\n",
    "    \n",
    "    for s in [nltk.tokenize.word_tokenize(s) for s in sentences]:\n",
    "        sentence_idx += 1\n",
    "        word_idx = []\n",
    "        \n",
    "        #For each word in the word list...\n",
    "        for w in important_words:\n",
    "            try:\n",
    "                #Compute an index for where any important word occurs in the sentence.\n",
    "                word_idx.append(s.index(w))\n",
    "            except ValueError as e: # w not in this particular sentence\n",
    "                pass\n",
    "            \n",
    "        word_idx.sort()\n",
    "        \n",
    "        #It is possible that some sentences may not contain any important words at all.\n",
    "        if len(word_idx) == 0: continue\n",
    "            \n",
    "        #Using the word index, compute clusters by using a max distance threshold for any two consecutive words\n",
    "        \n",
    "        clusters = []\n",
    "        cluster = [word_idx[0]]\n",
    "        i = 1\n",
    "        while i < len(word_idx):\n",
    "            if word_idx[i] - word_idx[i-1] < CLUSTER_THRESHOLD:\n",
    "                cluster.append(word_idx[i])\n",
    "            else:\n",
    "                clusters.append(cluster[:])\n",
    "                cluster = [word_idx[i]]\n",
    "            i += 1\n",
    "        clusters.append(cluster)\n",
    "        \n",
    "        #Scores for each cluster. The max score for any given cluster is the score for the sentence.\n",
    "        \n",
    "        max_cluster_score = 0\n",
    "        for c in clusters:\n",
    "            significant_words_in_cluster = len(c)\n",
    "            total_words_in_cluster = c[-1] - c[0] + 1\n",
    "            score = 1.0 * significant_words_in_cluster \\\n",
    "                    * significant_words_in_cluster / total_words_in_cluster\n",
    "            \n",
    "            if score > max_cluster_score:\n",
    "                max_cluster_score = score\n",
    "                \n",
    "        scores.append((sentence_idx, score))\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize(txt):\n",
    "    txt = txt.replace(\"\\n\",\"\")\n",
    "    sentences = [s for s in nltk.tokenize.sent_tokenize(txt)]\n",
    "    normalized_sentences = [s.lower() for s in sentences]\n",
    "    words = [w.lower() for sentence in normalized_sentences for w in nltk.tokenize.word_tokenize(sentence)]\n",
    "    fdist = nltk.FreqDist(words)\n",
    "    top_n_words = [w[0] for w in fdist.items()\n",
    "                  if w[0] not in nltk.corpus.stopwords.words('english')][:]\n",
    "    scored_sentences = _score_sentences(normalized_sentences, top_n_words)\n",
    "\n",
    "    #Summarization Approach 1:\n",
    "    #Filter out nonsignificant sentences by using the average scores plus a fraction of the std dev as a filter\n",
    "\n",
    "    avg = np.mean([s[1] for s in scored_sentences])\n",
    "    std = np.std([s[1] for s in scored_sentences])\n",
    "    mean_scored = [(sent_idx, score) for (sent_idx, score) in scored_sentences \n",
    "                   if score > avg + 0.5 * std]\n",
    "\n",
    "    #Summarization Approach 2:\n",
    "    #Another approach would be to return only the top N ranked sentences\n",
    "\n",
    "    top_n_scored = sorted(scored_sentences, key=lambda s: s[1])[-TOP_SENTENCES:]\n",
    "    top_n_scored = sorted(top_n_scored, key=lambda s:s[0])\n",
    "    \n",
    "    return dict(top_n_summary = [sentences[idx] for (idx, score) in top_n_scored], \n",
    "               mean_scored_summary = [sentences[idx] for (idx, score) in mean_scored])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "text1 = pdf_to_text(\"/Users/itziar/Downloads/DS_NLP_Assignment_final/WC500057122.pdf\")\n",
    "text2 = pdf_to_text(\"/Users/itziar/Downloads/DS_NLP_Assignment_final/WC500135744.pdf\")\n",
    "\n",
    "section1 = get_specific_section(text1, 'Non-clinical aspects')\n",
    "section2 = get_specific_section(text2, 'Non-clinical aspects')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#summarize(section1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#summarize(section2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
